% term_paper.tex

% preamble
\documentclass[10pt,letterpaper,notitlepage,draft]{article}

%\usepackage[latin1]{inputenc}
%\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{eufrak}
\usepackage{eucal}
\usepackage{amsthm}
\usepackage{array}
\usepackage[final]{listings}
%\usepackage{tikz}

% define theorems
\theoremstyle{definition}
\newtheorem{my_thm}{Theorem}
\newtheorem{my_clm}{Claim}
\newtheorem{my_ex}{Example}
\newtheorem*{my_def}{Definition}
\newtheorem*{my_note}{Note}

% make q.e.d. symbol black square
\renewcommand\qedsymbol{
    \ensuremath{\blacksquare}
}

% listings settings
\lstset{
    basicstyle=\ttfamily,
    numbers=left,
    numberstyle=\small,
%   showspaces=true,
%   showtabs=true,
    mathescape=true,
    frame=single
}

% my title is a command for title and author
% #1 title
% #2 author
%\newcommand\mytitle[2]{
%    \parbox{0.9\textwidth}{
%        \centering\bfseries\large #1\\
%        By #2\vspace{\baselineskip}
%    }
%}

\setlength\extrarowheight{2pt}

\vfuzz2pt % don't report over-full v-boxes if over-edge is small
\hfuzz2pt % don't report over-full h-boxes if over-edge is small

\pagestyle{empty}

\title{CS 527 Fall 2014\\Term Paper}
\author{\small
\small Spencer Hubbard\\
\small Oregon State University\\
\small Corvallis, OR 97331\\
\small hubbarsp@onid.oregonstate.edu
% 2nd. author
\and
\small William Leslie\\
\small Oregon State University\\
\small Corvallis, OR 97331\\
\small lesliew@onid.oregonstate.edu
% 3rd. author
\and
\small Francis Vo\\
\small Oregon State University\\
\small Corvallis, OR 97331\\
\small vof@onid.oregonstate.edu
} % TODO: add other author's names on new line
\date{} % TODO: change date to actual due date

\begin{document}

% front matter
\maketitle

% TODO: write abstract
%\begin{abstract}
%\end{abstract}

% main matter
\section{Introduction}
For each $i \in \lbrace 1, \ldots, n\rbrace$, let $x_i$ be a symbol and let $p_i$ be the probability associated with $x_i$. 
Let $f_i = \sum_{j=1}^{i-1} p_j + p_i / 2$ and let $\ell_i = \lceil\log 1 / p_i \rceil + 1$. 
Now, let $c_i$ be the first $\ell_i$ bits in the binary expansion of the number $q_i$. 
Shannon-Fano-Elias code uses $c_i$ as the codeword for $x_i$. 
It can be shown that the length of the codewords satisfy Kraft's inequality, i.e., $\sum_{i=1}^n 2^{-\ell_i} \le 1$, which means that the code is uniquely decodable. 
It can also be shown that the code is a prefix code. 
Note that we do not assume that the symbols are indexed by decreasing probability. 
When the symbols are indexed by decreasing probability, this is just Shannon-Fano code.

Suppose that Alice wants to send a secret message to Bob but their adversary Eve---the eavesdropper---is allowed to intercept and attempt to read any message that Alice sends Bob. 
Suppose also that Alice, Bob, and Eve all know the set of symbols and their associated probabilities. 
Notice that for a fixed set of $n$ symbols and associated probabilities, there are $n!$ Shannon-Fano-Elias codes corresponding to the $n!$ permutations of the indices $\lbrace 1, \dots, n\rbrace$. 
This means if Alice and Bob can agree upon a particular permutation without Eve knowing, then Alice can use Shannon-Fano-Elias code to send a message to Bob and Eve must potentially check $n!$ permutations before successfully reading the message.

\section{Huffman Code vs Shannon Code}
% The downsides of huffman codes and why it can't be used for encryption
Huffman code is an optimal prefix code and could be used for data compression, but it cannot be used for encryption.
Since the probability of each symbol is very important to the encryption, knowing fixed set of $n$ symbols, associated probabilities, and the algorithm used will easily yield the correct encoding.
Shannon-Fano-Elias Coding is extremely sub-optimal, but great for encryption.
Even with knowledge of the fixed set of $n$ symbols, associated probabilities, and the algorithm used, there are still $n!$ different encodings. 
Shannon-Fano-Elias Coding depends greatly on the order of the the symbols as well as the probabilities.


\section{Improved Shannon-Fano-Elias Coding}

\subsection{"Better" PMF for encoding}
%This improvement to Shannon-Fano-Elias code aims to change the PMF used for the encoding. Since Huffman coding is more optimial, we will use the length of each symbol to generate a new PMF.
Since the symbols are not necessarily indexed by decreasing probability, it follows that the resulting code may have a large average length.
To decrease the average length of the code, we will replace the true probabilities with some temporary probabilities and construct the code using these temporary probabilities.

For each $i \in \lbrace 1, \ldots, n\rbrace$, let $x_i$ and $p_i$ be defined as before.
Let $\ell_i$ be the length of the Huffman codeword for the symbol $x_i$ with probability $p_i$ and let $L$ be the average length of this Huffman code, i.e., let $L = \sum_{i=1}^n p_i \ell_i$.
Now, let $p_i^\prime = 2^{-\ell_i}$ be the temporary probability of the symbol $x_i$ and let $\ell_i^\prime$ be the length of the Shannon-Fano-Elias codeword for the symbol $x_i$ with the temporary probability $p_i^\prime$, i.e., let $\ell_i^\prime =  \lceil\log_2 1 / p_i^\prime \rceil + 1$.
Also, let $L^\prime$ be the average length of this Shannon-Fano-Elias code with respect to the true probabilities, i.e., let $L^\prime = \sum_{i=1}^n p_i \ell_i^\prime$.
Notice that $\ell_i^\prime = \ell_i + 1$.
This means
\begin{align*}
L^\prime
&= \sum_{i=1}^n p_i \ell_i^\prime \\
%&= \sum_{i=1}^n p_i \left(\lceil\log 1 / p_i^\prime \rceil + 1\right) \\
&= \sum_{i=1}^n p_i \left(\ell_i + 1\right) \\
&= \sum_{i=1}^n p_i \ell_i + \sum_{i=1}^n p_i \\
&= L + 1
\end{align*}
which means the average length of this Shannon-Fano-Elias code is within one of optimal.

% Another reason to use the better PMF 
Using the the Shannon-Fano-Elias Code in Table~\ref{t1}, we will generate a better PMF. First we have to generate a Huffman code of the symbols. We find out that each codeword should have length 2 and therefore each $p_i^\prime = 0.25$. We will then use $p_i^\prime$, to generate a new Shannon-Fano-Elias code.

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|l|}
\hline
Symbols & $p_i$ & $f_i$ & Codeword \\ 
\hline 
\hline
D & 1/4 & 1/8 & 001 \\
\hline
C & 1/6 & 1/3 & 0101 \\
\hline
B & 1/4 & 13/24 & 100 \\
\hline
A & 1/3 & 5/6 & 110 \\
\hline
\end{tabular}
\end{center}
\caption{Base Shannon-Fano-Elias Code}\label{t1}
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|l|}
\hline
Symbols & $p_i^\prime$ & $f_i^\prime$ & Codeword \\ 
\hline 
\hline
D & 1/4 & 1/8 & 001 \\
\hline
C & 1/4 & 3/8 & 011 \\
\hline
B & 1/4 & 5/8 & 101 \\
\hline
A & 1/4 & 7/8 & 111 \\
\hline
\end{tabular}
\end{center}
\caption{Shannon-Fano-Elias Code using better PMF}\label{t2}
\end{table}


\subsection{Codeword Truncation}
In Figure~\ref{f1}, the rightmost bits of each codeword are truncated while it and its preceding and succeeding codewords are mutually prefix-free. Since the algorithm removes bits from the codeword, it will reduce the average code length and therefore optimize the code. This works because the closer codes are, the more similar their prefixes are due to the codeword being based on the running probability. Therefore, the codeword does not have to be compared to every other codeword and only the ones immediately before and after it.

 \begin{figure}[h]
\caption{Figure text here.}\label{f1}
\begin{lstlisting}[breaklines]
Input: Shannon-Fano-Elias codeword set codeword($x_i$) for 1$\le$i$\le$n.
Output: codeword($x_i$) with lower expected length.
    for 1$\le$i$\le$n do
        c($x_i$) $\leftarrow$ codeword($x_i$)
        while codeword(xi-1),c($x_i$) and codeword($x_i+1$) are mutually prefix-free do
            codeword($x_i$) $\leftarrow$ c($x_i$)
            c($x_i$) $\leftarrow$ removing the rightmost bit of the codeword($x_i$)
        end while
    end for
\end{lstlisting}
\end{figure} 

Using the algorithm in order on Table~\ref{t1}, we have 
001 compared to 0101.  
These codewords are mutually prefix-free and the rightmost bit can be deleted.  
D is now 00.  
Removing one more would result in 0, which is a prefix of 0101 and the algorithm continues to the next codeword.

0101 is compared to 00 and 100.
These codewords are prefix-free and the rightmost bit can be deleted.
C is now 010.
010 is compared to 00 and 100.
These codewords are still prefix-free and the rightmost bit can be deleted.
C is now 01.
Deleting one more would result in 0 and therefore a prefix of 00, thus the algorithm can move to the next codeword.

100 is compared to 01 and 110.
These codewords are prefix-free and the rightmost bit can be deleted.
B is now 10.
Deleting one more bit would result in 1, which is a prefix of 110 and therefore the algorithm moves on.

110 is compared to 10.
These codewords are prefix-free and the rightmost bit can be deleted.
A is now 11.
Deleting one more bit would result in 1, which is a prefix of 10, therefore the algorithm continues.
The algorithm then ends because our codeword set has been exhausted.

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Symbols & $p_i$ & $f_i$ & Codeword \\
\hline
\hline
D & 1/4 & 1/8 & 00 \\
\hline
C & 1/6 & 1/3 & 01 \\
\hline
B & 1/4 & 13/24 & 10 \\
\hline
A & 1/3 & 5/6 & 11 \\
\hline
\end{tabular}
\end{center}
\caption{Caption text here}\label{t3}
\end{table}

Our resulting code is given in table~\ref{t3}.
Here we see a significantly reduced code. 
The average length has been reduced to 2, as opposed to the 3.2 that it was previously. 
Not only is this code significantly reduced in this case, it is also optimal. 
Thus, in certain cases, this truncation method can result in an optimal Huffman code.

\section{Finding the Codeword Set Used for Encoding}
Cryptanalysis is very hard for Shannon-Fano-Elias Code because there are $n!$ different encodings. 
Decryption uses a $O(n^3)$ algorithm to check if the codeword set could have been used to generate the sequence. 
We will need to use this algorithm for all sets of codewords. There are $n!$ different sets of codewords. 
The algorithm will create a subset of sets of codewords that work on the sequence.
Another sequence can be used to make a smaller subset.

\subsection{Example 1}\label{s1}
Given the received encoded sequence and a candidate codeword set {11110,110,01}. The codeword set needs to be listed in order by length.\\

\makebox[.9\linewidth][c]{Received encoded sequence: 0101010100100101000001000010101010010000011000001}\\

The codeword 11110 doesn't appear in the received encoded sequence, so we have to search for the codeword 110. 
The bold numbers represent the codeword 110 that were found, and the underlined numbers are codewords that are found before the codeword 110.\\

\makebox[.9\linewidth][c]{Searching for 110 in sequence 01010101001001010000010000101010100100000{\bf 110}00001}\\

For every codeword, we will delete it from the received encoded sequence if any other codewords (that we haven't checked) appear before it. 
But codewords {110,01} were not found before the codeword 110, so the current sequence is still unchanged from received encoded sequence. Now we search for the codeword 01. \\

\makebox[.9\linewidth][c]{Searching for 01 in sequence: {\bf \underline {010101}01}0{\bf 01}0{\bf\underline{01}01}0000{\bf 01}000{\bf\underline{010101}01}0{\bf 01}0000{\bf 01}10000{\bf 01}}\\

After searching for all occurrences of the codeword 01, we remove the occurrences that have the codewords {01} to the left of it\\

\makebox[.9\linewidth][c]{After removing 01 from sequence: 001001000001000010010000011000001}\\

Since the sequence is not empty, after we searched using on the codewords.

\subsection{Example 2}
Once again, we are still using received encoded sequence from example 1 in Section \ref{s1}.
This time we are checking if the codeword set {00001,010,10}.
Fist we will search for the longest codeword in the set in the sequence.\\

\makebox[.9\linewidth][c]{Searching for 00001 in sequence: 01010101001001\underline{010}{\bf\underline{00001}}{\bf00001}0101010\underline{010}{\bf00001}\underline{10}{\bf00001}}\\


We found 4 occurrences of the codeword 00001, and they each have another codeword from the set {00001,010,10} to the left of it. 
Therefore we will delete all of them. 
We will continue searching though the new sequence for the next codeword 010.\\

\makebox[.9\linewidth][c]{Searching for 010 in sequence after removing all 00001: {\bf 0101010\underline{10}\underline{010}010\underline{10}01010\underline{10}01010}}\\

We found 12 occurrences of the codeword 010, but not all of them had other codewords from the set {010,10} before it. 
So we only deleted 4 occurrences.
We will search for the last codeword 10.\\

\makebox[.9\linewidth][c]{Searching for 10 in sequence after removing all 010: {\bf \underline{101010101010}10}}\\

There are only codeword 10 remaining, and therefore the final sequence is empty, and the codeword set {00001,010,10} may be the set used.

% TODO: write section
\section{Conclusion}

% back matter
\nocite{*}
\bibliographystyle{amsplain}
%\bibliography{reference}
% \bibliographystyle{plain}
\bibliography{term_paper}

\end{document}
